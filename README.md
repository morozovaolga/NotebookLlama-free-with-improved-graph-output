
# NotebookLlaMaü¶ô

## –§–æ—Ä–∫ –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ [Llama](https://github.com/run-llama/notebookllama)

–≠—Ç–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π ‚Äî –º–æ–π –∫–∞—Å—Ç–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–∫ –ø—Ä–æ–µ–∫—Ç–∞ Llama. –Ø —Ä–µ—à–∏–ª–∞ —Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º, —É–±—Ä–∞—Ç—å –≤—Å–µ –ø–ª–∞—Ç–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —É–ª—É—á—à–∏—Ç—å –≤—ã–≤–æ–¥ –º–∞–π–Ω–¥–º—ç–ø–æ–≤, –¥–∏–∞–≥—Ä–∞–º–º –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤.

**–û—Å–Ω–æ–≤–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è:**
- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ API –∏ –º–æ–¥–µ–ª–∏)
- –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –º–∞–π–Ω–¥–º—ç–ø–æ–≤, –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –¥–∏–∞–≥—Ä–∞–º–º
- –û—Ç–∫—Ä—ã—Ç—ã–π –∫–æ–¥ –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏

–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–µ–∫—Ç: [github.com/run-llama/notebookllama](https://github.com/run-llama/notebookllama)

---


### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

This project uses `uv` to manage dependencies. Before you begin, make sure you have `uv` installed.

On macOS and Linux:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

On Windows:

```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

For more install options, see `uv`'s [official documentation](https://docs.astral.sh/uv/getting-started/installation/).

---


### –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç


**1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**

```bash
git clone https://github.com/run-llama/notebookllama
cd notebookllama/
```


**2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**

```bash
uv sync
```


**3. –£–∫–∞–∂–∏—Ç–µ API-–∫–ª—é—á–∏**

First, create your `.env` file by renaming the example file:

```bash
mv .env.example .env
```

>
> - For **North America**: This is the default region - no configuration necesary.
> - For **Europe (EU)**: Uncomment and set `LLAMACLOUD_REGION="eu"`

–î–∞–ª–µ–µ –æ—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª `.env` –∏ –¥–æ–±–∞–≤—å—Ç–µ –≤–∞—à–∏ –∫–ª—é—á–∏:

- `HUGGINGFACE_API_KEY`: –ø–æ–ª—É—á–∏—Ç–µ –Ω–∞ [HuggingFace](https://huggingface.co/settings/tokens)
- `LLAMACLOUD_API_KEY`: –ø–æ–ª—É—á–∏—Ç–µ –Ω–∞ [LlamaCloud Dashboard](https://cloud.llamaindex.ai?utm_source=demo&utm_medium=notebookLM)

> **üåç –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: LlamaCloud —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö. –î–ª—è –ï–≤—Ä–æ–ø—ã —É–∫–∞–∂–∏—Ç–µ:
> - –î–ª—è **–ï–≤—Ä–æ–ø—ã (EU)**: `LLAMACLOUD_REGION="eu"`


**4. –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ**

(on mac/unix)

```bash
source .venv/bin/activate
```

(on Windows):

```bash
.\.venv\Scripts\activate
```


**5. –°–æ–∑–¥–∞–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ –∏ –ø–∞–π–ø–ª–∞–π–Ω LlamaCloud**

You will now execute two scripts to configure your backend agents and pipelines.

First, create the data extraction agent:

```bash
uv run tools/create_llama_extract_agent.py
```

Next, run the interactive setup wizard to configure your index pipeline.


> **‚ö° –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é HuggingFace):**
> –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≤—ã–±–µ—Ä–∏—Ç–µ **"With Default Settings"** ‚Äî –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –ø–∞–π–ø–ª–∞–π–Ω —Å –±–µ—Å–ø–ª–∞—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é HuggingFace (`sentence-transformers/paraphrase-multilingual-mpnet-base-v2`).

> **üß† –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º (—Å–≤–æ–∏ –º–æ–¥–µ–ª–∏):**
> –î–ª—è –≤—ã–±–æ—Ä–∞ –¥—Ä—É–≥–æ–π –±–µ—Å–ø–ª–∞—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ HuggingFace (–Ω–∞–ø—Ä–∏–º–µ—Ä, `all-MiniLM-L6-v2`, `bge-small-en-v1.5`), –≤—ã–±–µ—Ä–∏—Ç–µ **"With Custom Settings"** –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.

Run the wizard with the following command:

```bash
uv run tools/create_llama_cloud_index.py
```



**6. –ó–∞–ø—É—Å—Ç–∏—Ç–µ backend-—Å–µ—Ä–≤–∏—Å—ã**

–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å—Ç–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã Postgres, Jaeger, Adminer –∏ Ollama:

```bash
docker compose up -d
```

**7. –£–∫–∞–∂–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π –∏ –∑–∞–¥–∞—á**

–í —Ñ–∞–π–ª–µ `config.yaml` (–∏–ª–∏ `.env`) —É–∫–∞–∂–∏—Ç–µ –Ω—É–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LLM –∏ –∑–∞–¥–∞—á:

```yaml
llm:
	provider: ollama
	model: mistral
	endpoint: http://localhost:11434
	max_new_tokens: 1024
	temperature: 0.1
	top_p: 0.9

tasks:
	summary:
		model: mistral
	mindmap:
		model: mistral
	facts:
		model: mistral
```

**8. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ**

–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä:

```powershell
# Recommended: run as a module so package imports work (ensures `src` is on PYTHONPATH)
$env:PYTHONPATH = "src"; python -m notebookllama.server
```

If you prefer the simpler script form (may require adjusting PYTHONPATH), you can run:

```powershell
# Make sure `src` is on PYTHONPATH first, or run with the module form above
$env:PYTHONPATH = "src"; python src\notebookllama\server.py
```

–í –Ω–æ–≤–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Streamlit:

```powershell
python -m streamlit run src/notebookllama/Home.py
```

**–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Streamlit (Windows PowerShell):**
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É Streamlit, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É:

```powershell
Stop-Process -Name streamlit -Force
```
–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–∫—Ä–æ–π—Ç–µ –æ–∫–Ω–æ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞, –ª–∏–±–æ –Ω–∞–∂–º–∏—Ç–µ `Ctrl+C`.

> [!IMPORTANT]
>
> _You might need to install `ffmpeg` if you do not have it installed already_

Note on PDF parsing: for more reliable PDF extraction the project prefers `pdfplumber`.
If you experience truncated or failed PDF parsing, install it in your environment:

```powershell
pip install pdfplumber
```


–û—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ: `http://localhost:8501/`

---


### –í–∫–ª–∞–¥ –∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∞

Contribute to this project following the [guidelines](./CONTRIBUTING.md).


### –õ–∏—Ü–µ–Ω–∑–∏—è

This project is provided under an [MIT License](./LICENSE).
